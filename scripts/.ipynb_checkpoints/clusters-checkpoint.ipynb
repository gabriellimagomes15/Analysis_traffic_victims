{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from somber import Som\n",
    "from sklearn.cluster import KMeans\n",
    "import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "import json\n",
    "\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "\n",
    "from fbprophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "(warnings.filterwarnings(\"ignore\", category=tqdm.TqdmSynchronisationWarning))\n",
    "\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CRIANDO FUNÇÕES\n",
    "\n",
    "def get_encod_label(df):\n",
    "    \"\"\" função para converter variáveis categóricas em numéricas com a função\n",
    "    LabelEncoder()\n",
    "    \"\"\"\n",
    "    print(\"Transformação (LabelEncoder) ...\")\n",
    "    \n",
    "    encod_label     = list()\n",
    "    df_encod_labels = pd.DataFrame()\n",
    "\n",
    "    for i in df.columns:\n",
    "        encoder = LabelEncoder()\n",
    "        print(i)\n",
    "        df_encod_labels[i+\"Label\"] = encoder.fit_transform(df[i].astype(str))\n",
    "        encod_label.append({i+\"Label\": encoder})\n",
    "\n",
    "    return encod_label, df_encod_labels\n",
    "\n",
    "def get_unique_cluster(result, col):\n",
    "    \"\"\" função para ter cluster único para cada registro\"\"\"\n",
    "    print(\"Get Uniques Cluster\")\n",
    "    \n",
    "    count = result.groupby([col,\"cluster\"]).cluster.count()\n",
    "    df    = pd.DataFrame(count)\n",
    "\n",
    "    lista_cluster = list()\n",
    "    for i in df.index.levels[0]:\n",
    "        lista_cluster.append(df.loc[[i],\"cluster\"].idxmax())\n",
    "\n",
    "    cluster_estados = pd.DataFrame(lista_cluster,columns=[col,\"cluster\"])\n",
    "\n",
    "    lista_cluster_acid = list()\n",
    "    for i in cluster_estados.to_dict(orient='index').items():\n",
    "        lista_cluster_acid.append(i[1])\n",
    "        \n",
    "    return lista_cluster_acid\n",
    "\n",
    "def get_desc_cluster(encod_label,cluster):\n",
    "    \"\"\" Função para recuperar características do cluster (dados que formam determinado cluster)\"\"\"\n",
    "    print(\"Get Desc Cluster\")\n",
    "\n",
    "    desc_cluster = {} \n",
    "    clust = 0\n",
    "    lista_desc_cluster = list()\n",
    "\n",
    "    for centers in cluster.cluster_centers_:\n",
    "        i = 0\n",
    "        print(\"  Cluster: \", clust)\n",
    "        desc = list()\n",
    "        for c in centers:\n",
    "            for k, v in encod_label[i].items():\n",
    "                desc.append({k.replace(\"Label\",\"\"):v.classes_.item(int(c))})\n",
    "            i += 1\n",
    "        desc_cluster[str(clust)] = desc        \n",
    "        clust += 1\n",
    "        \n",
    "    lista_desc_cluster.append(desc_cluster)        \n",
    "    return lista_desc_cluster\n",
    "\n",
    "\n",
    "def run_ml(X_train,X_test,y_train,y_test):\n",
    "    list_clf = list()\n",
    "    list_col = list()\n",
    "    list_auc = list()\n",
    "    text_clf = None\n",
    "    \n",
    "    modelos = [ SGDClassifier(),#loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=15, tol=None),\n",
    "                #SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=15, tol=None),\n",
    "               SVC(),#C= 10, gamma= 0.001, kernel= 'linear', probability= False),\n",
    "                RandomForestClassifier(),#criterion='entropy', max_depth= None, max_features='sqrt',n_estimators= 10000),\n",
    "                LogisticRegression(),#solver='lbfgs',multi_class='multinomial',class_weight='balanced',random_state=0),\n",
    "                DecisionTreeClassifier(),#max_depth=20, min_samples_split=50), #81.3%\n",
    "                ExtraTreesClassifier(),#n_estimators=10, max_depth=None,min_samples_split=2, random_state=0), #80.6%\n",
    "                AdaBoostClassifier(),#base_estimator=SVC(),\n",
    "                                   #learning_rate=1.0,n_estimators=400,algorithm='SAMME'),#n_estimators=10),\n",
    "                GradientBoostingClassifier(),#n_estimators=1000, learning_rate=1.0,max_depth=1, random_state=0),\n",
    "                GaussianNB(),\n",
    "                #BernoulliNB(),\n",
    "                MultinomialNB(),\n",
    "               KNeighborsClassifier(),#n_neighbors=11),\n",
    "               BaggingClassifier(),#RandomForestClassifier(),max_samples=0.5,max_features=0.5)\n",
    "                ]\n",
    "\n",
    "    ## VERIFINCANDO IMPORTANCIAS DOS ATRIBUTOS\n",
    "\n",
    "    for m in modelos:\n",
    "        print(\"\\n==>\",m.__class__.__name__)\n",
    "        \n",
    "        # model\n",
    "        text_clf = m\n",
    "        \n",
    "        # Modelo criado com GridSearchCV\n",
    "        if grid:\n",
    "            print(\"  GRID\")\n",
    "            ini = time.time()\n",
    "\n",
    "            df_grid = X_treino.copy() #.loc[:,:]\n",
    "            df_grid[\"classe\"] = y_treino\n",
    "            df_grid = df_grid.sample(int(len(df_grid)*.31))\n",
    "            \n",
    "            gs_cv = GridSearchCV(text_clf, parametros[m.__class__.__name__], \n",
    "                                 scoring = 'neg_mean_squared_error', n_jobs = 6).fit(df_grid.iloc[:,:-1], \n",
    "                                                                                     df_grid.loc[:,\"classe\"])\n",
    "\n",
    "            print(\"   TOTAL GRID: \", (time.time() - ini)/60 )        \n",
    "            print(\"   Best params: \", gs_cv.best_params_)\n",
    "\n",
    "            text_clf.set_params(**gs_cv.best_params_)\n",
    "            #text_clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"  CLF FIT \")\n",
    "        ini = time.time()\n",
    "        \n",
    "        text_clf  = text_clf.fit(np.asarray(X_train), y_train)\n",
    "        print(\"   TOTAL FIT: \", (time.time() - ini)/60 )        \n",
    "\n",
    "        predicted = text_clf.predict(np.asarray(X_test))\n",
    "        print(\"  Accuracy  = %f \\n\" % accuracy_score(y_test, predicted))\n",
    "        \n",
    "        \n",
    "        cm     = confusion_matrix(y_test, predicted,labels=labels)    \n",
    "        plot_cm(cm,labels)\n",
    "        \n",
    "        #return text_clf\n",
    "    \n",
    "\"\"\"        \n",
    "        if hasattr(text_clf, 'predict_proba'):\n",
    "            y_pred_proba = text_clf.predict_proba(X_test)[::,1]\n",
    "            fpr, tpr, _  = sklearn.metrics.roc_curve(y_test,  y_pred_proba)\n",
    "            auc          = sklearn.metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "            plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "            plt.legend(loc=4)\n",
    "            plt.show()\n",
    "\n",
    "    print(\"================================================================================================\")\n",
    "\n",
    "\n",
    "    for m in modelos:\n",
    "        print(\"\\n==>\",m.__class__.__name__)\n",
    "        cols = X_train.columns\n",
    "        m.fit(np.asarray(X_train),y_train)\n",
    "\n",
    "        if hasattr(m, 'feature_importances_'):\n",
    "            x_labels = X_train.columns[:]\n",
    "            prob     = list()        \n",
    "            cols     = list()        \n",
    "            features = m.feature_importances_\n",
    "            for i,l in zip(x_labels,features):\n",
    "                if l*100 > int( np.mean(features)*100 ):\n",
    "                    cols.append(i)\n",
    "            print(\"  Cols: \", cols)\n",
    "        \n",
    "        text_clf  = m.fit(np.asarray(X_train.loc[:,cols]), y_train)\n",
    "        predicted = text_clf.predict(np.asarray(X_test.loc[:,cols]))\n",
    "\n",
    "        print(\"  Accuracy  = %f \\n\" % accuracy_score(y_test, predicted))\n",
    "        cm     = confusion_matrix(y_test, predicted,labels=labels)    \n",
    "        plot_cm(cm,labels)\n",
    "        \n",
    "        i = len(resultado)\n",
    "        resultado.loc[i,\"clf\"]  = m.__class__.__name__\n",
    "        resultado.loc[i,\"cols\"] = cols\n",
    "        resultado.loc[i,\"auc\"]  = accuracy_score(y_test, predicted)\n",
    "\n",
    "        if hasattr(text_clf, 'predict_proba'):\n",
    "            y_pred_proba = text_clf.predict_proba(X_test.loc[:,cols])[::,1]\n",
    "            fpr, tpr, _  = sklearn.metrics.roc_curve(y_test,  y_pred_proba)\n",
    "            auc          = sklearn.metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "            plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "            plt.legend(loc=4)\n",
    "            plt.show()\n",
    "\"\"\"\n",
    "\n",
    "def plot_cm(cm, labels):\n",
    "    \n",
    "    # Compute percentanges\n",
    "    percent = (cm*100.0)/np.array(np.matrix(cm.sum(axis=1)).T)  # Derp, I'm sure there's a better way\n",
    "    \n",
    "    print('Confusion Matrix Stats')\n",
    "    for i, label_i in enumerate(labels):\n",
    "        for j, label_j in enumerate(labels):\n",
    "            print(\"%s/%s: %.2f%% (%d/%d)\" % (label_i, label_j, (percent[i][j]), cm[i][j], cm[i].sum()) )\n",
    "\n",
    "    # Show confusion matrix\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.grid(b=False)\n",
    "    cax = ax.matshow(percent, cmap='coolwarm')\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def predict_data(data, clf): #test_it\n",
    "    _alexa_match = alexa_counts * alexa_vc.transform(data).T\n",
    "    _dict_match  = dict_counts * dict_vc.transform(data).T\n",
    "    lens = [ len(d) for d in data ]\n",
    "    \n",
    "    for i,d in enumerate(domain):\n",
    "        _X = np.array( [ [lens[i], _alexa_match[i], _dict_match[i]] ])\n",
    "        print ('%s : %s' % (domain[i], clf.predict(_X)) )\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## IMPORTANDO OS DADOS QUE ESTÃO COMPACTADOS EM FORMATO .ZIP\n",
    "\n",
    "path = \"/home/gabriel.gomes/traffic_analytics/\"\n",
    "rf   = zipfile.ZipFile(path + \"dadosClean.zip\")\n",
    "for file in rf.infolist():\n",
    "    #print(file)\n",
    "    dados = pd.read_csv(rf.open(file),encoding=\"latin-1\",sep=\",\")#,nrows=1000)\n",
    "\n",
    "print(\"FINALIZADO!!!\", len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dados.loc[ ~(dados[\"uf\"].isin([\"RJ\",\"MG\"])),[\"uf\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## convertendo todos os campos para minúsculo\n",
    "dados = dados.applymap(lambda x: str(x).lower())\n",
    "\n",
    "## eliminando registros que possuem valores inválidos\n",
    "dados = dados.loc[(dados.uf != '(null)') & (dados.causa_acidente != '(null)') & (dados.tipo_acidente != '(null)') &\n",
    "                  (dados.fase_dia != '(null)') & ( (dados.sexo == 'masculino') | (dados.sexo == 'feminino') ) & \n",
    "                  (dados.estado_fisico != 'ignorado') & (dados.estado_fisico != 'nao informado')  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convertendo todos os campos para minúsculo\n",
    "dados = dados.applymap(lambda x: str(x).lower())\n",
    "\n",
    "## eliminando registros que possuem valores inválidos\n",
    "for c in dados.columns:\n",
    "    if c not in ['id', 'pesid', 'horario', 'dt','municipio','tipo_veiculo','ano_fabricacao',\n",
    "                'mes', 'ano', 'catIdade']:\n",
    "        dados = dados.loc[~(dados[c].isin(['(null)', 'ignorado', 'nao informado', 'inválido', 'não informado'])), :]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Counter(dados.sexo)\n",
    "len(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## FAZENDO UMA ANÁLISE EXPLORATÓRIA DOS DADOS\n",
    "for c in dados.columns:\n",
    "    if c not in ['id', 'pesid', 'horario', 'uf', 'dt','municipio','tipo_veiculo','ano_fabricacao',\n",
    "                'mes', 'ano', 'catIdade']:\n",
    "        print(\"\\n ==>\", c)\n",
    "        print(\"  \", Counter(dados[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CRIANDO SUBSETS COM DADOS SOBRE ACIDENTES E SOBRE AS VITIMAS\n",
    "dados_acidente = dados.loc[:,[\"uf\",\"day\",\"causa_acidente\",\"tipo_acidente\",\"fase_dia\",\"tipo_veiculo\"]]\n",
    "dados_vitima   = dados.loc[:,[\"uf\",\"tipo_envolvido\",\"estado_fisico\",\"idade\",\"sexo\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformação (LabelEncoder) ...\n",
      "day\n",
      "causa_acidente\n",
      "tipo_acidente\n",
      "fase_dia\n",
      "tipo_veiculo\n"
     ]
    }
   ],
   "source": [
    "## CRIANDO LABELS PARA OS REGISTROS\n",
    "label_acidentes, df_encod_acid = get_encod_label(dados_acidente.iloc[:,1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DEFININDO A QUANTIDADE DE CLUSTER (K) PARA DADOS DE ACIDENTES COM ELBOW\n",
    "\n",
    "distortions = []\n",
    "K = range(1,30)\n",
    "\n",
    "X = df_encod_acid.sample( int(len(df_encod_acid) * 0.3) ) ## UTILIZANDO UMA AMOSTRA DOS DADOS\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "    kmeanModel.fit(X)\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Uniques Cluster\n",
      "Get Desc Cluster\n",
      "  Cluster:  0\n",
      "  Cluster:  1\n",
      "  Cluster:  2\n",
      "Cluster Acidentes Finalizado!!!\n"
     ]
    }
   ],
   "source": [
    "## CLUSTER DAS CARACTERÍSTICAS DOS ACIDENTES\n",
    "\n",
    "# cluster com algoritmo KMeans\n",
    "cluster       = KMeans(n_clusters = 3,  max_iter = 500,init = 'random').fit(X=df_encod_acid)\n",
    "\n",
    "# recuperando cluster para cada UF\n",
    "cluster_acid  = dados_acidente.loc[:,[\"uf\"]]\n",
    "cluster_acid[\"cluster\"] = cluster.labels_\n",
    "\n",
    "# recuperando cluster únicos para cada UF e descrição de cada cluster\n",
    "list_clust_uf           = get_unique_cluster(cluster_acid,\"uf\")\n",
    "lista_desc_cluster      = get_desc_cluster(label_acidentes, cluster)\n",
    "\n",
    "# criando dict com cluster de acidentes para cada UF\n",
    "cluster_acidents = {'cluster_acidentes': list_clust_uf,\n",
    "                    'desc_cluster': lista_desc_cluster\n",
    "                   }\n",
    "\n",
    "## salvando clusters em json\n",
    "#with open('cluster_acidentes.json','w') as file:        json.dump(cluster_acidents, file)\n",
    "\n",
    "print(\"Cluster Acidentes Finalizado!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_acidentes': [{'uf': 'ac', 'cluster': 1},\n",
       "  {'uf': 'al', 'cluster': 1},\n",
       "  {'uf': 'am', 'cluster': 2},\n",
       "  {'uf': 'ap', 'cluster': 1},\n",
       "  {'uf': 'ba', 'cluster': 2},\n",
       "  {'uf': 'ce', 'cluster': 1},\n",
       "  {'uf': 'df', 'cluster': 1},\n",
       "  {'uf': 'es', 'cluster': 1},\n",
       "  {'uf': 'go', 'cluster': 2},\n",
       "  {'uf': 'ma', 'cluster': 1},\n",
       "  {'uf': 'mg', 'cluster': 2},\n",
       "  {'uf': 'ms', 'cluster': 1},\n",
       "  {'uf': 'mt', 'cluster': 1},\n",
       "  {'uf': 'pa', 'cluster': 1},\n",
       "  {'uf': 'pb', 'cluster': 1},\n",
       "  {'uf': 'pe', 'cluster': 1},\n",
       "  {'uf': 'pi', 'cluster': 1},\n",
       "  {'uf': 'pr', 'cluster': 1},\n",
       "  {'uf': 'rj', 'cluster': 2},\n",
       "  {'uf': 'rn', 'cluster': 1},\n",
       "  {'uf': 'ro', 'cluster': 1},\n",
       "  {'uf': 'rr', 'cluster': 1},\n",
       "  {'uf': 'rs', 'cluster': 1},\n",
       "  {'uf': 'sc', 'cluster': 1},\n",
       "  {'uf': 'se', 'cluster': 1},\n",
       "  {'uf': 'sp', 'cluster': 2},\n",
       "  {'uf': 'to', 'cluster': 1}],\n",
       " 'desc_cluster': [{'0': [{'day': 'quinta'},\n",
       "    {'causa_acidente': 'fenã´menos da natureza'},\n",
       "    {'tipo_acidente': 'colisã£o transversal'},\n",
       "    {'fase_dia': 'plena noite'},\n",
       "    {'tipo_veiculo': 'nã£o identificado'}],\n",
       "   '1': [{'day': 'segunda'},\n",
       "    {'causa_acidente': 'desobediãªncia ã\\xa0s normas de trã¢nsito pelo pedestre'},\n",
       "    {'tipo_acidente': 'colisã£o transversal'},\n",
       "    {'fase_dia': 'plena noite'},\n",
       "    {'tipo_veiculo': 'bicicleta'}],\n",
       "   '2': [{'day': 'quinta'},\n",
       "    {'causa_acidente': 'outras'},\n",
       "    {'tipo_acidente': 'colisã£o transversal'},\n",
       "    {'fase_dia': 'plena noite'},\n",
       "    {'tipo_veiculo': 'bicicleta'}]}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_acidents\n",
    "#with open('cluster_acidentes.json','w') as file:        json.dump(cluster_acidents, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformação (LabelEncoder) ...\n",
      "tipo_envolvido\n",
      "estado_fisico\n",
      "idade\n",
      "sexo\n"
     ]
    }
   ],
   "source": [
    "## CRIANDO LABELS PARA OS REGISTROS\n",
    "label_vitimas, df_encod_vit = get_encod_label(dados_vitima.iloc[:,1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## DEFININDO A QUANTIDADE DE CLUSTER (K) PARA DADOS DE VÍTIMAS COM ELBOW\n",
    "distortions = []\n",
    "K = range(1,5)\n",
    "\n",
    "X = df_encod_vit.sample( int(len(df_encod_acid) * 0.3) ) ## UTILIZANDO UMA AMOSTRA DOS DADOS\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "    kmeanModel.fit(X)\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Uniques Cluster\n",
      "Get Desc Cluster\n",
      "  Cluster:  0\n",
      "  Cluster:  1\n",
      "  Cluster:  2\n",
      "  Cluster:  3\n",
      "Cluster Vitimas Finalizado!!!\n"
     ]
    }
   ],
   "source": [
    "## CLUSTER DAS CARACTERÍSTICAS DOS ACIDENTES\n",
    "cluster       = KMeans(n_clusters= 4, max_iter = 500).fit(X=df_encod_vit)\n",
    "\n",
    "cluster_vit  = dados_vitima.loc[:,[\"uf\"]]\n",
    "cluster_vit[\"cluster\"] = cluster.labels_\n",
    "\n",
    "list_clust_uf           = get_unique_cluster(cluster_vit,\"uf\")\n",
    "lista_desc_cluster      = get_desc_cluster(label_vitimas, cluster)\n",
    "\n",
    "cluster_vitimas = {'cluster_acidentes': list_clust_uf,\n",
    "                   'desc_cluster': lista_desc_cluster\n",
    "                   }\n",
    "\n",
    "## salvando clusters em json\n",
    "#with open('cluster_vitimas.json','w') as file:        json.dump(cluster_vitimas, file)\n",
    "    \n",
    "print(\"Cluster Vitimas Finalizado!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_acidentes': [{'uf': 'ac', 'cluster': 2},\n",
       "  {'uf': 'al', 'cluster': 2},\n",
       "  {'uf': 'am', 'cluster': 0},\n",
       "  {'uf': 'ap', 'cluster': 2},\n",
       "  {'uf': 'ba', 'cluster': 3},\n",
       "  {'uf': 'ce', 'cluster': 2},\n",
       "  {'uf': 'df', 'cluster': 2},\n",
       "  {'uf': 'es', 'cluster': 2},\n",
       "  {'uf': 'go', 'cluster': 2},\n",
       "  {'uf': 'ma', 'cluster': 2},\n",
       "  {'uf': 'mg', 'cluster': 2},\n",
       "  {'uf': 'ms', 'cluster': 2},\n",
       "  {'uf': 'mt', 'cluster': 3},\n",
       "  {'uf': 'pa', 'cluster': 2},\n",
       "  {'uf': 'pb', 'cluster': 2},\n",
       "  {'uf': 'pe', 'cluster': 2},\n",
       "  {'uf': 'pi', 'cluster': 2},\n",
       "  {'uf': 'pr', 'cluster': 2},\n",
       "  {'uf': 'rj', 'cluster': 3},\n",
       "  {'uf': 'rn', 'cluster': 2},\n",
       "  {'uf': 'ro', 'cluster': 2},\n",
       "  {'uf': 'rr', 'cluster': 2},\n",
       "  {'uf': 'rs', 'cluster': 2},\n",
       "  {'uf': 'sc', 'cluster': 2},\n",
       "  {'uf': 'se', 'cluster': 2},\n",
       "  {'uf': 'sp', 'cluster': 2},\n",
       "  {'uf': 'to', 'cluster': 3}],\n",
       " 'desc_cluster': [{'0': [{'tipo_envolvido': 'condutor'},\n",
       "    {'estado_fisico': 'ferido leve'},\n",
       "    {'idade': '1'},\n",
       "    {'sexo': 'feminino'}],\n",
       "   '1': [{'tipo_envolvido': 'condutor'},\n",
       "    {'estado_fisico': 'ferido leve'},\n",
       "    {'idade': '59'},\n",
       "    {'sexo': 'feminino'}],\n",
       "   '2': [{'tipo_envolvido': 'condutor'},\n",
       "    {'estado_fisico': 'ferido leve'},\n",
       "    {'idade': '26'},\n",
       "    {'sexo': 'feminino'}],\n",
       "   '3': [{'tipo_envolvido': 'condutor'},\n",
       "    {'estado_fisico': 'ferido leve'},\n",
       "    {'idade': '40'},\n",
       "    {'sexo': 'feminino'}]}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_vitimas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predição de vítimas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## CRIANDO CONJUNTO DE DADOS COM TIMESERIES\n",
    "count = Counter(dados.dt)\n",
    "\n",
    "lista_idx = list()\n",
    "valor = list()\n",
    "for i,x in dict(count).items():\n",
    "    lista_idx.append(i)\n",
    "    valor.append(x)\n",
    "\n",
    "## CRIANDO DATAFRAME COM TIMESERIES NO FORMATO QUE O PACOTE Prophet EXIGE\n",
    "df       = pd.DataFrame() \n",
    "df[\"ds\"] = lista_idx\n",
    "df['y']  = preprocessing.minmax_scale( np.reshape(valor,(-1,1) ))#, columns=cols) #\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CRIANDO MODELO DE PREDIÇÃO DE VÍTIMAS\n",
    "model = Prophet()\n",
    "model.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## FAZENDO DF PARA OS PRÓXIMO 750 DIAS\n",
    "future = model.make_future_dataframe(periods=750)\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## FAZENDO A PREDIÇÃO PARA VÍTIMAS DE ACIDENTES DE TRÂNSITO \n",
    "forecast = model.predict(future)\n",
    "forecast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PLOT COM PREDIÇÃO DE VITIMAS\n",
    "fig1 = model.plot(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PLOT PREDIÇÃO POR ANO, DIA E MÊS\n",
    "fig2 = model.plot_components(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predição de acidentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dados_acidentes = dados.drop_duplicates([\"id\",\"dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(dados), len(dados_acidentes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CRIANDO CONJUNTO DE DADOS COM TIMESERIES\n",
    "count_acid = Counter(dados_acidentes.dt)\n",
    "\n",
    "lista_idx = list()\n",
    "valor = list()\n",
    "for i,x in dict(count_acid).items():\n",
    "    lista_idx.append(i)\n",
    "    valor.append(x)\n",
    "\n",
    "## CRIANDO DATAFRAME COM TIMESERIES NO FORMATO QUE O PACOTE Prophet EXIGE\n",
    "df_acid       = pd.DataFrame() \n",
    "df_acid[\"ds\"] = lista_idx\n",
    "df_acid['y']  = valor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CRIANDO MODELO DE PREDIÇÃO DE VÍTIMAS\n",
    "model_acid = Prophet()\n",
    "model_acid.fit(df_acid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## FAZENDO DF PARA OS PRÓXIMO 750 DIAS\n",
    "future_acid = model_acid.make_future_dataframe(periods=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## FAZENDO A PREDIÇÃO PARA ACIDENTES DE TRÂNSITO \n",
    "forecast_acid = model_acid.predict(future_acid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PLOT COM PREDIÇÃO DE VITIMAS\n",
    "fig1 = model_acid.plot(forecast_acid)\n",
    "\n",
    "## PLOT PREDIÇÃO POR ANO, DIA E MÊS\n",
    "fig2 = model_acid.plot_components(forecast_acid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = model.plot(forecast)\n",
    "g = model_acid.plot(forecast_acid, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
